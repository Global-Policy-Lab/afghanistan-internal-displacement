{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add user specific python libraries to path\n",
    "import sys\n",
    "sys.path.insert(0, \"/home/smehra/local-packages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This non-commercial license of GraphLab Create for academic use is assigned to xtai@berkeley.edu and will expire on January 29, 2021.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] graphlab.cython.cy_server: GraphLab Create v2.1 started. Logging: /tmp/graphlab_server_1583778080.log\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "from datetime import timedelta  \n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "\n",
    "import graphlab as gl\n",
    "gl.set_runtime_config('GRAPHLAB_CACHE_FILE_LOCATIONS', '/data/tmp/smehra/tmp')\n",
    "gl.set_runtime_config('GRAPHLAB_DEFAULT_NUM_PYLAMBDA_WORKERS', 48)\n",
    "import migration_detector as md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "# setup logging to a specified file\n",
    "log_file = '/data/tmp/smehra/logs/displacement_location_segments_computation.log'\n",
    "logging.basicConfig(filename=log_file,\n",
    "                            filemode='a+',\n",
    "                            format='%(asctime)s %(levelname)s %(message)s',\n",
    "                            datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                            level=logging.DEBUG)\n",
    "logger=logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SPARK_CONF_DIR\"] = \"/data/tmp/spark/conf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'spark.driver.memory', u'50g'),\n",
       " (u'spark.repl.local.jars',\n",
       "  u'file:///data/tmp/spark/jars/mysql-connector-java-5.1.30-bin.jar'),\n",
       " (u'spark.sql.shuffle.partitions', u'1000'),\n",
       " (u'spark.jars', u'/data/tmp/spark/jars/mysql-connector-java-5.1.30-bin.jar'),\n",
       " (u'spark.app.name', u'afgh_project_smehra_hive_setup'),\n",
       " (u'spark.master', u'local[30]'),\n",
       " (u'spark.executor.extraJavaOptions', u'-XX:+UseG1GC'),\n",
       " (u'spark.executor.id', u'driver'),\n",
       " (u'spark.driver.port', u'35327'),\n",
       " (u'spark.app.id', u'local-1583778092907'),\n",
       " (u'spark.local.dir', u'/data/tmp/smehra/tmp'),\n",
       " (u'spark.serializer', u'org.apache.spark.serializer.KryoSerializer'),\n",
       " (u'spark.ui.port', u'4050'),\n",
       " (u'spark.sql.warehouse.dir', u'/data/tmp/hive_warehouse'),\n",
       " (u'spark.sql.catalogImplementation', u'hive'),\n",
       " (u'spark.rdd.compress', u'True'),\n",
       " (u'spark.serializer.objectStreamReset', u'100'),\n",
       " (u'spark.driver.maxResultSize', u'2g'),\n",
       " (u'spark.submit.deployMode', u'client'),\n",
       " (u'spark.driver.host', u'umtiti.ischool.berkeley.edu'),\n",
       " (u'spark.ui.showConsoleProgress', u'true'),\n",
       " (u'spark.ui.enabled', u'True')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "import random\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import HiveContext\n",
    "\n",
    "config = pyspark.SparkConf().setAll([('spark.ui.port', 4050), \n",
    "                                     ('spark.ui.enabled', True),\n",
    "                                     \n",
    "                                     # if running in local mode, driver will be only executor\n",
    "                                     # hence, give driver as much memory as possible if running in local mode\n",
    "                                     ('spark.driver.memory','50g'), \n",
    "                                     \n",
    "                                     # set up executor config if running in cluster or client mode\n",
    "                                     #('spark.executor.instances', '5'), \n",
    "                                     #('spark.executor.cores', '5'), \n",
    "                                     #('spark.executor.memory', '5g'), \n",
    "                                     #('spark.executor.memoryOverhead', '500m'),\n",
    "                                     \n",
    "                                     # more partitions means smaller partition size per task\n",
    "                                     # hence, would reduce memory load\n",
    "                                     ('spark.sql.shuffle.partitions', '1000'),\n",
    "                                     \n",
    "                                     # increase max result size if you are \"collecting\" big dataset \n",
    "                                     # driver will need more memory to collect\n",
    "                                     ('spark.driver.maxResultSize', '2g'),\n",
    "                                     \n",
    "                                     # set location spark should use for temporary data\n",
    "                                     ('spark.local.dir', '/data/tmp/smehra/tmp'),\n",
    "                                     # Set location of hive database\n",
    "                                     ('spark.sql.warehouse.dir', '/data/tmp/hive_warehouse'),\n",
    "                                     # Add mysql connector jar to use mysql as metastore service\n",
    "                                     ('spark.jars', '/data/tmp/spark/jars/mysql-connector-java-5.1.30-bin.jar'),\n",
    "                                    \n",
    "                                     # KryoSerializer is faster and more compact than the Java default serializer.\n",
    "                                     ('spark.serializer', 'org.apache.spark.serializer.KryoSerializer'),\n",
    "                                     \n",
    "                                     # G1GC overcomes the latency and throughput limitations with the old garbage collectors.\n",
    "                                     ('spark.executor.extraJavaOptions','-XX:+UseG1GC')])\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .enableHiveSupport() \\\n",
    "        .config(conf=config) \\\n",
    "        .master(\"local[30]\") \\\n",
    "        .appName(\"afgh_project_smehra_hive_setup\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Get the Hive Context\n",
    "hive = HiveContext(spark.sparkContext)\n",
    "\n",
    "spark.sparkContext._conf.getAll()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Divide Daily Modal Location data into csv based buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+----+-----+---------+-----------+\n",
      "|          userId|tower_group_id|Year|Month|DaySeries|district_id|\n",
      "+----------------+--------------+----+-----+---------+-----------+\n",
      "|0YrK2DB31ne4lAmW|           469|2015|    7|      838|       1612|\n",
      "|0YrK2DB33nW4lAmW|           489|2015|    7|      838|       1602|\n",
      "|0YrK2DB3415olAmW|           735|2015|    7|      838|       1401|\n",
      "|0YrK2DB36nyGlAmW|           288|2015|    7|      838|       2312|\n",
      "|0YrK2DB36wE4lAmW|           550|2015|    7|      838|       1601|\n",
      "+----------------+--------------+----+-----+---------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "daily_modal_locations = hive.sql('SELECT * FROM afghanistan.user_daily_modal_locations')\n",
    "#daily_modal_locations = daily_modal_locations.sample(fraction =  0.01, seed = 42)\n",
    "daily_modal_locations.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|    Date|DaySeries|\n",
      "+--------+---------+\n",
      "|20130401|        1|\n",
      "|20130402|        2|\n",
      "|20130403|        3|\n",
      "|20130404|        4|\n",
      "|20130405|        5|\n",
      "|20130406|        6|\n",
      "|20130407|        7|\n",
      "|20130408|        8|\n",
      "|20130409|        9|\n",
      "|20130410|       10|\n",
      "|20130411|       11|\n",
      "|20130412|       12|\n",
      "|20130413|       13|\n",
      "|20130414|       14|\n",
      "|20130415|       15|\n",
      "|20130416|       16|\n",
      "|20130417|       17|\n",
      "|20130418|       18|\n",
      "|20130419|       19|\n",
      "|20130420|       20|\n",
      "+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create dayseries_to_date map\n",
    "\n",
    "from datetime import timedelta  \n",
    "from datetime import date\n",
    "\n",
    "march_31_2013 = date(2013, 3, 31)\n",
    "\n",
    "dayseries_to_date = pd.DataFrame({})\n",
    "# for each user .. detect displacement segments\n",
    "for daySeries in range(1, 1462):\n",
    "\n",
    "    dateForDaySeries = int((march_31_2013 + timedelta(days = daySeries)).strftime(\"%Y%m%d\"))\n",
    "    dayseries_to_date = dayseries_to_date.append({'DaySeries': daySeries, 'Date': dateForDaySeries}, ignore_index = True)\n",
    "\n",
    "dayseries_to_date.Date = dayseries_to_date.Date.astype(int)\n",
    "dayseries_to_date.DaySeries = dayseries_to_date.DaySeries.astype(int)\n",
    "\n",
    "dayseries_to_date_sparkDF = spark.createDataFrame(dayseries_to_date)\n",
    "dayseries_to_date_sparkDF.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------+--------------+-----------+\n",
      "|         user_id|    date|tower_group_id|district_id|\n",
      "+----------------+--------+--------------+-----------+\n",
      "|0YrK2DB30543lAmW|20140718|          1331|        821|\n",
      "|0YrK2DB315R6lAmW|20140718|          1273|       3201|\n",
      "|0YrK2DB33eD6lAmW|20140718|            81|       2001|\n",
      "|0YrK2DB349J6lAmW|20140718|            95|       2001|\n",
      "|0YrK2DB34x16lAmW|20140718|           794|        101|\n",
      "|0YrK2DB35k1RlAmW|20140718|          1149|        308|\n",
      "|0YrK2DB36M36lAmW|20140718|          1110|        308|\n",
      "|0YrK2DB38km4lAmW|20140718|           571|       1615|\n",
      "|0YrK2DB394nWlAmW|20140718|          1208|       1212|\n",
      "|0YrK2DB3B5v4lAmW|20140718|           992|        301|\n",
      "|0YrK2DB3BVa3lAmW|20140718|           887|        101|\n",
      "|0YrK2DB3BWmrlAmW|20140718|          1168|        101|\n",
      "|0YrK2DB3BrW4lAmW|20140718|           796|        107|\n",
      "|0YrK2DB3G95RlAmW|20140718|          1114|        101|\n",
      "|0YrK2DB3JMWwlAmW|20140718|           109|       2001|\n",
      "|0YrK2DB3JzAwlAmW|20140718|           851|        101|\n",
      "|0YrK2DB3K8k6lAmW|20140718|            44|       2101|\n",
      "|0YrK2DB3X35wlAmW|20140718|           279|       1801|\n",
      "|0YrK2DB3ZNpwlAmW|20140718|          1053|        101|\n",
      "|0YrK2DB3eZGWlAmW|20140718|           507|       1601|\n",
      "+----------------+--------+--------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "daily_modal_locations_formatted = daily_modal_locations.select([F.col(\"userId\").alias(\"user_id\"),\n",
    "                                                                F.col(\"tower_group_id\"),\n",
    "                                                                F.col(\"district_id\"),\n",
    "                                                                F.col(\"DaySeries\")])\n",
    "\n",
    "daily_modal_locations_formatted = daily_modal_locations_formatted.join(dayseries_to_date_sparkDF, \n",
    "                                                                       daily_modal_locations_formatted.DaySeries == dayseries_to_date_sparkDF.DaySeries, \n",
    "                                                                       how = 'left') \\\n",
    "                                                                 .select([\"user_id\", \"date\", \"tower_group_id\", \"district_id\"])\n",
    "\n",
    "daily_modal_locations_formatted.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'7', u'D', u'd', u'R', u'9', u'j', u'L', u'z', u'y', u'G', u'b', u'g', u'3', u'0', u'k', u'v', u'J', u'P', u'r', u'm', u'n', u'W', u'1', u'E', u'e', u'o']\n"
     ]
    }
   ],
   "source": [
    "user_id_first_char_list = daily_modal_locations_formatted.withColumn(\"user_id_first_char\", daily_modal_locations_formatted.user_id.substr(0, 1))\n",
    "user_id_first_char_list = user_id_first_char_list.select(\"user_id_first_char\").distinct().toPandas()\n",
    "print(user_id_first_char_list.user_id_first_char.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering on .. 7\n",
      "Filtering on .. D\n",
      "Filtering on .. d\n",
      "Filtering on .. R\n",
      "Filtering on .. 9\n",
      "Filtering on .. j\n",
      "Filtering on .. L\n",
      "Filtering on .. z\n",
      "Filtering on .. y\n",
      "Filtering on .. G\n",
      "Filtering on .. b\n",
      "Filtering on .. g\n",
      "Filtering on .. 3\n",
      "Filtering on .. 0\n",
      "Filtering on .. k\n",
      "Filtering on .. v\n",
      "Filtering on .. J\n",
      "Filtering on .. P\n",
      "Filtering on .. r\n",
      "Filtering on .. m\n",
      "Filtering on .. n\n",
      "Filtering on .. W\n",
      "Filtering on .. 1\n",
      "Filtering on .. E\n",
      "Filtering on .. e\n",
      "Filtering on .. o\n"
     ]
    }
   ],
   "source": [
    "daily_modal_districts_formatted = daily_modal_locations_formatted.select([F.col(\"user_id\"),\n",
    "                                                                          F.col(\"date\"),\n",
    "                                                                          F.col(\"district_id\").alias(\"location\")])\n",
    "\n",
    "for first_char in user_id_first_char_list.user_id_first_char.tolist():\n",
    "    print('Filtering on .. ' + first_char)\n",
    "    \n",
    "    subset_of_users = daily_modal_districts_formatted.filter(daily_modal_districts_formatted.user_id.substr(0, 1) == first_char)\n",
    "    subset_of_users = subset_of_users.sort([\"user_id\", \"date\"])\n",
    "    subset_of_users.write.option(\"header\",\"true\").csv('/data/tmp/smehra/aggregated_data/migration_detector_input_data/district_level/' + first_char)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering on .. 7\n",
      "Filtering on .. D\n",
      "Filtering on .. d\n",
      "Filtering on .. R\n",
      "Filtering on .. 9\n",
      "Filtering on .. j\n",
      "Filtering on .. L\n",
      "Filtering on .. z\n",
      "Filtering on .. y\n",
      "Filtering on .. G\n",
      "Filtering on .. b\n",
      "Filtering on .. g\n",
      "Filtering on .. 3\n",
      "Filtering on .. 0\n",
      "Filtering on .. k\n",
      "Filtering on .. v\n",
      "Filtering on .. J\n",
      "Filtering on .. P\n",
      "Filtering on .. r\n",
      "Filtering on .. m\n",
      "Filtering on .. n\n",
      "Filtering on .. W\n",
      "Filtering on .. 1\n",
      "Filtering on .. E\n",
      "Filtering on .. e\n",
      "Filtering on .. o\n"
     ]
    }
   ],
   "source": [
    "daily_modal_towers_formatted = daily_modal_locations_formatted.select([F.col(\"user_id\"),\n",
    "                                                                          F.col(\"date\"),\n",
    "                                                                          F.col(\"tower_group_id\").alias(\"location\")])\n",
    "\n",
    "for first_char in user_id_first_char_list.user_id_first_char.tolist():\n",
    "    print('Filtering on .. ' + first_char)\n",
    "    \n",
    "    subset_of_users = daily_modal_towers_formatted.filter(daily_modal_towers_formatted.user_id.substr(0, 1) == first_char)\n",
    "    subset_of_users = subset_of_users.sort([\"user_id\", \"date\"])\n",
    "    subset_of_users.write.option(\"header\",\"true\").csv('/data/tmp/smehra/aggregated_data/migration_detector_input_data/tower_level/' + first_char)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Use Migration Detector to detect segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>Using default 48 lambda workers.</pre>"
      ],
      "text/plain": [
       "Using default 48 lambda workers."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>To maximize the degree of parallelism, add the following code to the beginning of the program:</pre>"
      ],
      "text/plain": [
       "To maximize the degree of parallelism, add the following code to the beginning of the program:"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>\"graphlab.set_runtime_config('GRAPHLAB_DEFAULT_NUM_PYLAMBDA_WORKERS', 56)\"</pre>"
      ],
      "text/plain": [
       "\"graphlab.set_runtime_config('GRAPHLAB_DEFAULT_NUM_PYLAMBDA_WORKERS', 56)\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Note that increasing the degree of parallelism also increases the memory footprint.</pre>"
      ],
      "text/plain": [
       "Note that increasing the degree of parallelism also increases the memory footprint."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: Detecting migration\n",
      "Done\n",
      "Start: Detecting migration\n",
      "Done\n",
      "Start: Detecting migration\n",
      "Done\n",
      "Start: Detecting migration\n",
      "Done\n",
      "Start: Detecting migration\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "def get_results_dataset_column_list():\n",
    "    column_list = ['userId']\n",
    "    march_31_2013 = date(2013, 3, 31)\n",
    "\n",
    "    # for each user .. detect displacement segments\n",
    "    for daySeries in range(1, 1462):\n",
    "\n",
    "        dateForDaySeries = (march_31_2013 + timedelta(days = daySeries)).strftime(\"%Y%m%d\")\n",
    "        column_list.append(daySeries)\n",
    "\n",
    "    return column_list\n",
    "\n",
    "# we use daily modal locations as input data to migration_detector\n",
    "# intermediate input dataset divides users into buckets based on first char of their user id\n",
    "for first_char_of_user_id in ['0', '1', '3', '7', '9', 'b', 'd', 'D', 'e', 'E', 'g', 'G', 'j', 'J', 'k', 'L', 'm', 'n', 'o', 'P', 'r', 'R', 'v', 'W', 'y', 'z']:\n",
    "    \n",
    "    # intermediate data is available at district_level or tower_level\n",
    "    level = 'district_level'\n",
    "\n",
    "    logger.info('processing users with first character of userId: ' + str(first_char_of_user_id))\n",
    "    input_path = '/data/tmp/smehra/aggregated_data/migration_detector_input_data/' + level + '/' + str(first_char_of_user_id)\n",
    "    \n",
    "    user_daily_modal_loc = gl.SFrame.read_csv(input_path, verbose = False)\n",
    "    logger.info('shape of input set: ' + str(user_daily_modal_loc.shape))\n",
    "    \n",
    "    unique_users_list = user_daily_modal_loc['user_id'].unique()\n",
    "    unique_users_count = str(len(unique_users_list))\n",
    "    logger.info('total unique users in this set: ' + unique_users_count)\n",
    "\n",
    "    logger.info('Migration Detector: creating trajectory record..')\n",
    "    traj = md.createTrajRecordFromSFrame(user_daily_modal_loc)\n",
    "\n",
    "    logger.info('Migration Detector: finding migrants..')\n",
    "    migrants = traj.find_migrants(num_stayed_days_migrant=5, \n",
    "                                  num_days_missing_gap=2,\n",
    "                                  small_seg_len=7, \n",
    "                                  seg_prop=0.5, \n",
    "                                  min_overlap_part_len=0,\n",
    "                                  max_gap_home_des=1470)\n",
    "\n",
    "    logger.info('Migration Detector: fetching segments..')\n",
    "    segments = traj.get_segments_as_pandas_DF(which_step=3)\n",
    "\n",
    "    logger.info('cleaning segments dataset..')\n",
    "    segments['segment_start_date'] = pd.to_datetime(segments['segment_start_date'], format='%Y%m%d').dt.date\n",
    "    segments['segment_end_date'] = pd.to_datetime(segments['segment_end_date'], format='%Y%m%d').dt.date\n",
    "\n",
    "    # get start and end of segments in form of day series\n",
    "    segments['start'] = (segments['segment_start_date'] - date(2013, 3, 31)).dt.days\n",
    "    segments['end'] = (segments['segment_end_date'] - date(2013, 3, 31)).dt.days\n",
    "\n",
    "    segments['segment_length'] = segments.segment_length.astype(int)\n",
    "\n",
    "    logger.info('creating an empty dataframe to store results..')\n",
    "    detected_locations = pd.DataFrame(columns = get_results_dataset_column_list())\n",
    "    \n",
    "    # ingest a list of unique users into results dataset\n",
    "    detected_locations['userId'] = unique_users_list\n",
    "\n",
    "    logger.info('processing segments..')\n",
    "    start_time = int(round(time.time()))\n",
    "\n",
    "    # iterate through each user\n",
    "    for index_outer, user in detected_locations.iterrows():\n",
    "\n",
    "        logger.info('processing user number: ' + str(index_outer))\n",
    "        \n",
    "        # iterate through each segment of user\n",
    "        for index_inner, segment in segments[segments.user_id == user.userId].iterrows():\n",
    "            \n",
    "            # update results dataset with detected segment\n",
    "            user[segment.start : (segment.end + 1)] = segment.location\n",
    "\n",
    "    end_time = int(round(time.time()))\n",
    "    logger.info('finished processing all users in ' + str((end_time - start_time)/60) + ' minutes')\n",
    "    \n",
    "    detected_locations.to_csv('/data/tmp/smehra/aggregated_data/migration_detector_output_data/' + level + '/' + str(first_char_of_user_id) + '.csv', index = False)\n",
    "    logger.info('finished saving dataset for users with first character of userId: ' + str(first_char_of_user_id))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3. Ingest output into hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write datafram to hive\n",
    "hive.sql('DROP TABLE afghanistan.migration_based_user_districts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(database=u'afghanistan', tableName=u'raw_data_phone_calls', isTemporary=False),\n",
       " Row(database=u'afghanistan', tableName=u'user_daily_modal_districts_wide', isTemporary=False),\n",
       " Row(database=u'afghanistan', tableName=u'user_daily_modal_locations_long', isTemporary=False),\n",
       " Row(database=u'afghanistan', tableName=u'user_daily_modal_towers_wide', isTemporary=False),\n",
       " Row(database=u'afghanistan', tableName=u'user_daily_unique_districts_long', isTemporary=False),\n",
       " Row(database=u'afghanistan', tableName=u'user_daily_unique_districts_wide', isTemporary=False),\n",
       " Row(database=u'afghanistan', tableName=u'user_daily_unique_towers_long', isTemporary=False),\n",
       " Row(database=u'afghanistan', tableName=u'user_daily_unique_towers_wide', isTemporary=False)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hive.sql('use afghanistan')\n",
    "hive.sql('show tables').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "migration_based_user_districts = spark.read.csv('/data/tmp/smehra/aggregated_data/migration_detector_output_data/district_level/*.csv',header = True, inferSchema=True)\n",
    "migration_based_user_districts.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write datafram to hive\n",
    "migration_based_user_districts.write.saveAsTable('afghanistan.migration_based_user_districts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from Hive\n",
    "user_segments = hive.sql('SELECT * FROM migration_based_user_districts')\n",
    "user_segments.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10477778"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_segments.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
