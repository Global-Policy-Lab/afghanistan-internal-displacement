{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/smehra/local-packages', '', '/home/smehra/.conda/envs/smehra_py2/lib/python27.zip', '/home/smehra/.conda/envs/smehra_py2/lib/python2.7', '/home/smehra/.conda/envs/smehra_py2/lib/python2.7/plat-linux2', '/home/smehra/.conda/envs/smehra_py2/lib/python2.7/lib-tk', '/home/smehra/.conda/envs/smehra_py2/lib/python2.7/lib-old', '/home/smehra/.conda/envs/smehra_py2/lib/python2.7/lib-dynload', '/home/smehra/.local/lib/python2.7/site-packages', '/home/smehra/.conda/envs/smehra_py2/lib/python2.7/site-packages', '/home/smehra/.conda/envs/smehra_py2/lib/python2.7/site-packages/IPython/extensions', '/home/smehra/.ipython']\n"
     ]
    }
   ],
   "source": [
    "# Add user specific python libraries to path\n",
    "import sys\n",
    "sys.path.insert(0, \"/home/smehra/local-packages\")\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pyspark.sql.functions as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SPARK_CONF_DIR\"] = \"/data/tmp/spark/conf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'spark.driver.memory', u'50g'),\n",
       " (u'spark.sql.shuffle.partitions', u'1000'),\n",
       " (u'spark.driver.extraJavaOptions',\n",
       "  u'-Dderby.system.home=/data/tmp/hive_warehouse/derby_metastore_service'),\n",
       " (u'spark.master', u'local[30]'),\n",
       " (u'spark.executor.extraJavaOptions', u'-XX:+UseG1GC'),\n",
       " (u'spark.executor.id', u'driver'),\n",
       " (u'spark.local.dir', u'/data/tmp/smehra/tmp'),\n",
       " (u'spark.serializer', u'org.apache.spark.serializer.KryoSerializer'),\n",
       " (u'spark.ui.port', u'4050'),\n",
       " (u'spark.sql.warehouse.dir', u'/data/tmp/hive_warehouse'),\n",
       " (u'spark.driver.port', u'46004'),\n",
       " (u'spark.sql.catalogImplementation', u'hive'),\n",
       " (u'spark.rdd.compress', u'True'),\n",
       " (u'spark.serializer.objectStreamReset', u'100'),\n",
       " (u'spark.submit.deployMode', u'client'),\n",
       " (u'spark.app.name', u'smehra_afgh_project'),\n",
       " (u'spark.app.id', u'local-1581446432883'),\n",
       " (u'spark.driver.host', u'umtiti.ischool.berkeley.edu'),\n",
       " (u'spark.ui.showConsoleProgress', u'true'),\n",
       " (u'spark.ui.enabled', u'True')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "import random\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import HiveContext\n",
    "\n",
    "config = pyspark.SparkConf().setAll([('spark.ui.port', 4050), \n",
    "                                     ('spark.ui.enabled', True),\n",
    "                                     \n",
    "                                     # if running in local mode, driver will be only executor\n",
    "                                     # hence, give driver as much memory as possible if running in local mode\n",
    "                                     ('spark.driver.memory','50g'), \n",
    "                                     \n",
    "                                     # set up executor config if running in cluster or client mode\n",
    "                                     #('spark.executor.instances', '5'), \n",
    "                                     #('spark.executor.cores', '5'), \n",
    "                                     #('spark.executor.memory', '5g'), \n",
    "                                     #('spark.executor.memoryOverhead', '500m'),\n",
    "                                     \n",
    "                                     # more partitions means smaller partition size per task\n",
    "                                     # hence, would reduce memory load\n",
    "                                     ('spark.sql.shuffle.partitions', '1000'),\n",
    "                                     \n",
    "                                     # increase max result size if you are \"collecting\" big dataset \n",
    "                                     # driver will need more memory to collect\n",
    "                                     #('spark.driver.maxResultSize', '2g'),\n",
    "                                     \n",
    "                                     # set location spark should use for temporary data\n",
    "                                     ('spark.local.dir', '/data/tmp/smehra/tmp'),\n",
    "                                     # Set location of hive database\n",
    "                                     ('spark.sql.warehouse.dir', '/data/tmp/hive_warehouse'),\n",
    "                                     # Add mysql connector jar to use mysql as metastore service\n",
    "                                     ('spark.jars', '/data/tmp/spark/jars/mysql-connector-java-5.1.30-bin.jar'),\n",
    "                                    \n",
    "                                     # KryoSerializer is faster and more compact than the Java default serializer.\n",
    "                                     ('spark.serializer', 'org.apache.spark.serializer.KryoSerializer'),\n",
    "                                     \n",
    "                                     # G1GC overcomes the latency and throughput limitations with the old garbage collectors.\n",
    "                                     ('spark.executor.extraJavaOptions','-XX:+UseG1GC')])\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .enableHiveSupport() \\\n",
    "        .config(conf=config) \\\n",
    "        .master(\"local[30]\") \\\n",
    "        .appName(\"smehra_afgh_project\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Get the Hive Context\n",
    "hive = HiveContext(spark.sparkContext)\n",
    "\n",
    "spark.sparkContext._conf.getAll()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------+---------+----------------+---------+---------+-----------+-------------------+---------+-------------+---------+---------------+----------------+----------+------+---------+--------+---------+\n",
      "|      phoneHash1|numtype1|ctrycode1|      phoneHash2| numtype2|ctrycode2|interaction|           datetime|yearmonth|call_duration|call_cost|     antenna_id|charged_duration|product_id|f_type|f_subtype|pay_type|subcos_id|\n",
      "+----------------+--------+---------+----------------+---------+---------+-----------+-------------------+---------+-------------+---------+---------------+----------------+----------+------+---------+--------+---------+\n",
      "|7orJ23R7GEYKqV1b|  mobile|       93|6LzVvQam1jvQMdG5|shortcode|       93|       call|2014-10-04 20:22:24|  2014-10|            9|      0.0|412204210242333|               0|        15|    SC|       SC|       0|   400003|\n",
      "|9zgKqvx9LoxjQWve|  mobile|       93|LNyd29oEmzOdl8zP|   mobile|       93|       call|2014-10-04 20:22:24|  2014-10|            4|     0.25|412204110520835|              15|    507000| ONNET|    ONNET|       0|   400001|\n",
      "|305xqe0kZ0RVlXGg|  mobile|       93|mOKlk5Aw1a4ME2p1|     intl|      971|       call|2014-10-04 20:22:24|  2014-10|          740|     65.0|412203810238053|             780|    504009|  INTL|     INTL|       0|   400007|\n",
      "|y4rZqRp9JabjQDMK|  mobile|       93|J305xqe80PglXGgW|shortcode|       93|       call|2014-10-04 20:22:24|  2014-10|          193|      0.0|412201010628586|               0|        15|    SC|       SC|       0|   400003|\n",
      "|edyL2yx10Vz0qjA8|  mobile|       93|3kEwqYmeW535qpJN|   mobile|       93|       call|2014-10-04 20:22:24|  2014-10|          185|    11.38|412204110420356|             195|    701006|OFFNET|    ETAFG|       0|   400001|\n",
      "+----------------+--------+---------+----------------+---------+---------+-----------+-------------------+---------+-------------+---------+---------------+----------------+----------+------+---------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read from Hive\n",
    "raw_data_phone_calls = hive.sql('SELECT * FROM afghanistan.raw_data_phone_calls')\n",
    "raw_data_phone_calls.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our methodology assumes a day starts at 5am and ends on 5am next day\n",
    "# Hence, we calculate \"effective\" date and time respectively.\n",
    "\n",
    "# Example:\n",
    "# Actual datetime: 2013-04-21 3.40am\n",
    "# Effective date: 2013-04-20\n",
    "# Effective hour: 23nd hour of the day\n",
    "\n",
    "raw_data_phone_calls_with_effective_time = raw_data_phone_calls.withColumn('effective_datetime', F.col('datetime') - F.expr(\"INTERVAL 5 HOURS\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------+---------------+\n",
      "|         user_id|    date|     antenna_id|\n",
      "+----------------+--------+---------------+\n",
      "|7orJ23R7GEYKqV1b|20141004|412204210242333|\n",
      "|9zgKqvx9LoxjQWve|20141004|412204110520835|\n",
      "|305xqe0kZ0RVlXGg|20141004|412203810238053|\n",
      "|y4rZqRp9JabjQDMK|20141004|412201010628586|\n",
      "|edyL2yx10Vz0qjA8|20141004|412204110420356|\n",
      "+----------------+--------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# keep and reformat columns needed for migration detection algorithm\n",
    "\n",
    "raw_data_for_migration_detection = raw_data_phone_calls_with_effective_time.withColumn('date', F.date_format(F.col(\"effective_datetime\"), \"YYYYMMdd\"))\n",
    "                                                                                                 \n",
    "raw_data_for_migration_detection = raw_data_for_migration_detection.select(F.col('phoneHash1').alias(\"user_id\"), \n",
    "                                                                 F.col('date'), \n",
    "                                                                 F.col(\"antenna_id\"))\n",
    "raw_data_for_migration_detection.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove null and invalid values\n",
    "raw_data_for_migration_detection_filtered = raw_data_for_migration_detection.filter(raw_data_for_migration_detection.user_id.isNotNull()\n",
    "                                                                                    & (raw_data_for_migration_detection.user_id != \"-99\")\n",
    "                                                                                    & raw_data_for_migration_detection.date.isNotNull()\n",
    "                                                                                    & (raw_data_for_migration_detection.date != \"-99\")\n",
    "                                                                                    & raw_data_for_migration_detection.antenna_id.isNotNull()\n",
    "                                                                                    & (raw_data_for_migration_detection.antenna_id != -99))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------+\n",
      "|tower_group_id|     antenna_id|\n",
      "+--------------+---------------+\n",
      "|           856|412200000000000|\n",
      "|           856|412200010229701|\n",
      "|           856|412200010229702|\n",
      "|           856|412200010229703|\n",
      "|           856|412200010229704|\n",
      "+--------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load tower to antenna mapping data\n",
    "tower_to_antenna_map = spark.read.csv('/data/projects/displacement_afghanistan/data/Aggregated_Groups/TowerDetails_WithGroupIDs_UTM42N.csv', header = True, inferSchema=True)\n",
    "tower_to_antenna_map = tower_to_antenna_map.select(F.col('Final_Agg_GroupID').alias(\"tower_group_id\"), F.col('callingcellid').alias(\"antenna_id\"))\n",
    "tower_to_antenna_map.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------+---------------+--------------+\n",
      "|         user_id|    date|     antenna_id|tower_group_id|\n",
      "+----------------+--------+---------------+--------------+\n",
      "|7orJ23R7GEYKqV1b|20141004|412204210242333|           345|\n",
      "|9zgKqvx9LoxjQWve|20141004|412204110520835|           123|\n",
      "|305xqe0kZ0RVlXGg|20141004|412203810238053|           676|\n",
      "|y4rZqRp9JabjQDMK|20141004|412201010628586|          1054|\n",
      "|edyL2yx10Vz0qjA8|20141004|412204110420356|            69|\n",
      "+----------------+--------+---------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# join daily modal location with tower to district mapping\n",
    "raw_data_with_tower_groups = raw_data_for_migration_detection_filtered.join(tower_to_antenna_map, \n",
    "                                                   raw_data_for_migration_detection_filtered.antenna_id == tower_to_antenna_map.antenna_id, \n",
    "                                                   how = 'left').select(raw_data_for_migration_detection_filtered['*'], tower_to_antenna_map['tower_group_id'])\n",
    "raw_data_with_tower_groups.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Daily Unique Towers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------+--------+\n",
      "|         user_id|    date|location|\n",
      "+----------------+--------+--------+\n",
      "|7orJ23R7GEYKqV1b|20141004|     345|\n",
      "|9zgKqvx9LoxjQWve|20141004|     123|\n",
      "|305xqe0kZ0RVlXGg|20141004|     676|\n",
      "|y4rZqRp9JabjQDMK|20141004|    1054|\n",
      "|edyL2yx10Vz0qjA8|20141004|      69|\n",
      "+----------------+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_data_with_tower_locations = raw_data_with_tower_groups.select(F.col('user_id'), \n",
    "                                                       F.col(\"date\"),\n",
    "                                                       F.col(\"tower_group_id\").alias('location'))\n",
    "raw_data_with_tower_locations.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop duplicates and sort data\n",
    "raw_data_with_tower_groups_deduped = raw_data_with_tower_locations.dropDuplicates()\n",
    "raw_data_with_tower_groups_deduped_sorted = raw_data_with_tower_groups_deduped.sort([\"user_id\", \"date\", \"location\"])\n",
    "\n",
    "# save in hive\n",
    "raw_data_with_tower_groups_deduped_sorted.write.saveAsTable('afghanistan.user_daily_unique_towers_long')\n",
    "\n",
    "user_daily_unique_towers_long = hive.sql('SELECT * FROM afghanistan.user_daily_unique_towers_long')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert long from to wide form dataset\n",
    "# one row per user\n",
    "# one column per day\n",
    "# value of each cell represent all towers user used on that day\n",
    "\n",
    "# note: collect_set dedups location values.\n",
    "# use collect_list if you need *all* location values for a day for a user\n",
    "user_daily_unique_towers_wide = user_daily_unique_towers_long.groupby('user_id').pivot('date').agg(F.collect_set('location'))\n",
    "user_daily_unique_towers_wide.show(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list of day series i.e from 20130101 to 20171231\n",
    "daySeriesList = set()\n",
    "\n",
    "for year in range(2013, 2018):\n",
    "    for month in [1, 3, 5, 7, 8, 10, 12]:\n",
    "        for day in range(1, 32):\n",
    "            daySeriesList.add(str(year) + (\"%02d\"%month) + (\"%02d\"%day))\n",
    "    \n",
    "    for month in [4, 6, 9, 11]:\n",
    "        for day in range(1, 31):\n",
    "            daySeriesList.add(str(year) + (\"%02d\"%month) + (\"%02d\"%day))\n",
    "            \n",
    "    for day in range(1, 30):\n",
    "        if(day == 29):\n",
    "            if(year%4 == 0):\n",
    "                daySeriesList.add(str(year) + \"02\" + (\"%02d\"%day))\n",
    "        else:\n",
    "            daySeriesList.add(str(year) + \"02\" + (\"%02d\"%day))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add empty columns for days for which we did not have any users making any calls\n",
    "\n",
    "# existing list of columns in user_daily_unique_towers_wide table\n",
    "existingDaySeriesColumns = user_daily_unique_towers_wide.columns\n",
    "existingDaySeriesColumns.remove('user_id')\n",
    "existingDaySeriesColumns = set(existingDaySeriesColumns)\n",
    "\n",
    "missingColumns = daySeriesList.difference(existingDaySeriesColumns)\n",
    "print('missing columns: ', missingColumns)\n",
    "\n",
    "for newColumn in missingColumns:\n",
    "    user_daily_unique_towers_wide = user_daily_unique_towers_wide.withColumn(str(newColumn), F.array())\n",
    "    print('added column: ', newColumn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save in hive\n",
    "user_daily_unique_towers_wide.write.saveAsTable('afghanistan.user_daily_unique_towers_wide')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Daily Unique Districts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+\n",
      "|tower_group_id|district_id|\n",
      "+--------------+-----------+\n",
      "|             0|       2008|\n",
      "|             1|       2008|\n",
      "|             2|       2007|\n",
      "|             3|       2008|\n",
      "|             4|       2008|\n",
      "+--------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load tower to district mapping data\n",
    "tower_to_district_map = spark.read.csv('/data/projects/displacement_afghanistan/data/Aggregated_Groups/Final_Aggregated_GroupIDs_UTM42N.csv', header = True, inferSchema=True)\n",
    "tower_to_district_map = tower_to_district_map.select(F.col('Final_Agg_GroupID').alias(\"tower_group_id\"), F.col('distid').alias(\"district_id\"))\n",
    "tower_to_district_map.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------+--------+\n",
      "|         user_id|    date|location|\n",
      "+----------------+--------+--------+\n",
      "|7orJ23R7GEYKqV1b|20141004|    1701|\n",
      "|9zgKqvx9LoxjQWve|20141004|    2001|\n",
      "|305xqe0kZ0RVlXGg|20141004|    2918|\n",
      "|y4rZqRp9JabjQDMK|20141004|     101|\n",
      "|edyL2yx10Vz0qjA8|20141004|    2001|\n",
      "+----------------+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_data_with_districts = raw_data_with_tower_groups.join(tower_to_district_map, \n",
    "                                                   raw_data_with_tower_groups.tower_group_id == tower_to_district_map.tower_group_id, \n",
    "                                                   how = 'left').select(raw_data_with_tower_groups['*'], tower_to_district_map['district_id'])\n",
    "\n",
    "raw_data_with_districts = raw_data_with_districts.select(F.col('user_id'), \n",
    "                                                       F.col(\"date\"),\n",
    "                                                       F.col(\"district_id\").alias('location'))\n",
    "raw_data_with_districts.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop duplicates and sort data\n",
    "raw_data_with_districts_deduped = raw_data_with_districts.dropDuplicates()\n",
    "raw_data_with_districts_deduped_sorted = raw_data_with_districts_deduped.sort([\"user_id\", \"date\", \"location\"])\n",
    "\n",
    "# save in hive\n",
    "raw_data_with_districts_deduped_sorted.write.saveAsTable('afghanistan.user_daily_unique_districts_long')\n",
    "\n",
    "user_daily_unique_districts_long = hive.sql('SELECT * FROM afghanistan.user_daily_unique_districts_long')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert long from to wide form dataset\n",
    "# one row per user\n",
    "# one column per day\n",
    "# value of each cell represent all towers user used on that day\n",
    "\n",
    "# note: collect_set dedups location values.\n",
    "# use collect_list if you need *all* location values for a day for a user\n",
    "\n",
    "user_daily_unique_districts_wide = user_daily_unique_districts_long.groupby('user_id').pivot('date').agg(F.collect_set('location'))\n",
    "user_daily_unique_districts_wide.show(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list of day series i.e from 20130101 to 20171231\n",
    "daySeriesList = set()\n",
    "\n",
    "for year in range(2013, 2018):\n",
    "    for month in [1, 3, 5, 7, 8, 10, 12]:\n",
    "        for day in range(1, 32):\n",
    "            daySeriesList.add(str(year) + (\"%02d\"%month) + (\"%02d\"%day))\n",
    "    \n",
    "    for month in [4, 6, 9, 11]:\n",
    "        for day in range(1, 31):\n",
    "            daySeriesList.add(str(year) + (\"%02d\"%month) + (\"%02d\"%day))\n",
    "            \n",
    "    for day in range(1, 30):\n",
    "        if(day == 29):\n",
    "            if(year%4 == 0):\n",
    "                daySeriesList.add(str(year) + \"02\" + (\"%02d\"%day))\n",
    "        else:\n",
    "            daySeriesList.add(str(year) + \"02\" + (\"%02d\"%day))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add empty columns for days for which we did not have any users making any calls\n",
    "\n",
    "# existing list of columns in user_daily_unique_towers_wide table\n",
    "existingDaySeriesColumns = user_daily_unique_districts_wide.columns\n",
    "existingDaySeriesColumns.remove('user_id')\n",
    "existingDaySeriesColumns = set(existingDaySeriesColumns)\n",
    "\n",
    "missingColumns = daySeriesList.difference(existingDaySeriesColumns)\n",
    "print('missing columns: ', missingColumns)\n",
    "\n",
    "for newColumn in missingColumns:\n",
    "    user_daily_unique_districts_wide = user_daily_unique_districts_wide.withColumn(str(newColumn), F.array())\n",
    "    print('added column: ', newColumn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save in hive\n",
    "user_daily_unique_districts_wide.write.saveAsTable('afghanistan.user_daily_unique_districts_wide')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-smehra_pyspark]",
   "language": "python",
   "name": "conda-env-.conda-smehra_pyspark-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
