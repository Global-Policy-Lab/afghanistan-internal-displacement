{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add user specific python libraries to path\n",
    "import sys\n",
    "sys.path.insert(0, \"/home/smehra/local-packages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "from datetime import timedelta  \n",
    "from datetime import date\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SPARK_CONF_DIR\"] = \"/data/tmp/spark/conf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'spark.driver.memory', u'50g'),\n",
       " (u'spark.repl.local.jars',\n",
       "  u'file:///data/tmp/spark/jars/mysql-connector-java-5.1.30-bin.jar'),\n",
       " (u'spark.sql.shuffle.partitions', u'1000'),\n",
       " (u'spark.app.id', u'local-1585092384719'),\n",
       " (u'spark.jars', u'/data/tmp/spark/jars/mysql-connector-java-5.1.30-bin.jar'),\n",
       " (u'spark.app.name', u'afgh_project_smehra_hive_setup'),\n",
       " (u'spark.master', u'local[30]'),\n",
       " (u'spark.executor.extraJavaOptions', u'-XX:+UseG1GC'),\n",
       " (u'spark.executor.id', u'driver'),\n",
       " (u'spark.driver.port', u'38473'),\n",
       " (u'spark.local.dir', u'/data/tmp/smehra/tmp'),\n",
       " (u'spark.serializer', u'org.apache.spark.serializer.KryoSerializer'),\n",
       " (u'spark.ui.port', u'4050'),\n",
       " (u'spark.kryoserializer.buffer.max.mb', u'2000'),\n",
       " (u'spark.sql.warehouse.dir', u'/data/tmp/hive_warehouse'),\n",
       " (u'spark.sql.catalogImplementation', u'hive'),\n",
       " (u'spark.rdd.compress', u'True'),\n",
       " (u'spark.serializer.objectStreamReset', u'100'),\n",
       " (u'spark.driver.maxResultSize', u'2g'),\n",
       " (u'spark.submit.deployMode', u'client'),\n",
       " (u'spark.driver.host', u'umtiti.ischool.berkeley.edu'),\n",
       " (u'spark.ui.showConsoleProgress', u'true'),\n",
       " (u'spark.ui.enabled', u'True')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "import random\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "config = pyspark.SparkConf().setAll([('spark.ui.port', 4050), \n",
    "                                     ('spark.ui.enabled', True),\n",
    "                                     \n",
    "                                     # if running in local mode, driver will be only executor\n",
    "                                     # hence, give driver as much memory as possible if running in local mode\n",
    "                                     ('spark.driver.memory','50g'), \n",
    "                                     \n",
    "                                     # set up executor config if running in cluster or client mode\n",
    "                                     #('spark.executor.instances', '5'), \n",
    "                                     #('spark.executor.cores', '5'), \n",
    "                                     #('spark.executor.memory', '5g'), \n",
    "                                     #('spark.executor.memoryOverhead', '500m'),\n",
    "                                     \n",
    "                                     # more partitions means smaller partition size per task\n",
    "                                     # hence, would reduce memory load\n",
    "                                     ('spark.sql.shuffle.partitions', '1000'),\n",
    "                                     \n",
    "                                     # increase max result size if you are \"collecting\" big dataset \n",
    "                                     # driver will need more memory to collect\n",
    "                                     ('spark.driver.maxResultSize', '2g'),\n",
    "                                     \n",
    "                                     # set location spark should use for temporary data\n",
    "                                     ('spark.local.dir', '/data/tmp/smehra/tmp'),\n",
    "                                     # Set location of hive database\n",
    "                                     ('spark.sql.warehouse.dir', '/data/tmp/hive_warehouse'),\n",
    "                                     # Add mysql connector jar to use mysql as metastore service\n",
    "                                     ('spark.jars', '/data/tmp/spark/jars/mysql-connector-java-5.1.30-bin.jar'),\n",
    "                                    \n",
    "                                     # KryoSerializer is faster and more compact than the Java default serializer.\n",
    "                                     ('spark.serializer', 'org.apache.spark.serializer.KryoSerializer'),\n",
    "                                     ('spark.kryoserializer.buffer.max.mb', '2000'),\n",
    "                                     \n",
    "                                     # G1GC overcomes the latency and throughput limitations with the old garbage collectors.\n",
    "                                     ('spark.executor.extraJavaOptions','-XX:+UseG1GC')])\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .enableHiveSupport() \\\n",
    "        .config(conf=config) \\\n",
    "        .master(\"local[30]\") \\\n",
    "        .appName(\"afgh_project_smehra_hive_setup\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Get the Hive Context\n",
    "hive = HiveContext(spark.sparkContext)\n",
    "\n",
    "spark.sparkContext._conf.getAll()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Number of users per district per day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detected Segments based user counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>district_id</th>\n",
       "      <th>day_series</th>\n",
       "      <th>user_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>1</td>\n",
       "      <td>86936.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101</td>\n",
       "      <td>2</td>\n",
       "      <td>100962.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101</td>\n",
       "      <td>3</td>\n",
       "      <td>107369.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101</td>\n",
       "      <td>4</td>\n",
       "      <td>113819.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>101</td>\n",
       "      <td>5</td>\n",
       "      <td>119551.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   district_id  day_series  user_count\n",
       "0          101           1     86936.0\n",
       "1          101           2    100962.0\n",
       "2          101           3    107369.0\n",
       "3          101           4    113819.0\n",
       "4          101           5    119551.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_segments_per_district_per_day = pd.read_csv('/data/tmp/smehra/aggregated_data/migration_detector_output_data/district_day_metrics.csv')\n",
    "user_segments_per_district_per_day.drop(columns = ['percentage_migrated'], inplace = True)\n",
    "user_segments_per_district_per_day.columns = ['district_id', 'day_series', 'user_count']\n",
    "user_segments_per_district_per_day.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[401, 402, 614, 708, 816, 1201, 1801, 2902, 3202, 3203]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random_districts = random.sample(user_segments_per_district_per_day.district_id.unique().tolist(), 10)\n",
    "random_districts = sorted([402, 1201, 816, 2902, 3203, 1801, 708, 614, 3202, 401])\n",
    "random_districts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "for district in random_districts:\n",
    "    \n",
    "    district_data = user_segments_per_district_per_day[(user_segments_per_district_per_day.district_id == district)]\n",
    "    ax.plot(district_data.day_series, district_data.user_count)\n",
    "    \n",
    "#ax.set_xticks([1,200,400,600,800,1000,1200])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daily Modal based user counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_daily_modal_districts = hive.sql('select * from afghanistan.user_daily_modal_districts_wide')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>district_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   district_id\n",
       "0          101\n",
       "1          102\n",
       "2          103\n",
       "3          104\n",
       "4          105"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "district_ids = [101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 201, 202, 203, 204, 205, 206, 207, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 401, 402, 403, 404, 405, 406, 407, 408, 409, 501, 502, 503, 504, 505, 506, 507, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 901, 902, 903, 904, 905, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615, 1701, 1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2101, 2102, 2103, 2104, 2105, 2106, 2107, 2108, 2109, 2110, 2111, 2201, 2202, 2203, 2204, 2205, 2301, 2302, 2303, 2304, 2305, 2306, 2307, 2308, 2309, 2310, 2311, 2312, 2313, 2401, 2402, 2403, 2404, 2405, 2406, 2407, 2408, 2409, 2410, 2411, 2412, 2413, 2414, 2415, 2416, 2501, 2502, 2503, 2504, 2505, 2506, 2507, 2508, 2509, 2510, 2511, 2601, 2603, 2604, 2604, 2605, 2701, 2702, 2703, 2704, 2705, 2706, 2707, 2708, 2709, 2710, 2801, 2802, 2803, 2804, 2805, 2806, 2807, 2901, 2902, 2903, 2904, 2905, 2906, 2907, 2908, 2909, 2910, 2911, 2912, 2913, 2914, 2915, 2916, 2917, 2918, 2919, 3001, 3002, 3003, 3004, 3005, 3006, 3007, 3008, 3101, 3102, 3103, 3104, 3105, 3106, 3107, 3201, 3202, 3203, 3204, 3205, 3206, 3207, 3208, 3209, 3210, 3211, 3212, 3213, 3301, 3302, 3303, 3304, 3305, 3306, 3307, 3401, 3402, 3403, 3404, 3405, 3406, 3407, 3408, 3409]\n",
    "\n",
    "# create an empty dataframe where we will store per district per day counts\n",
    "per_district_per_date_counts = pd.DataFrame(columns = ['district_id']) \n",
    "per_district_per_date_counts['district_id'] = district_ids\n",
    "per_district_per_date_counts.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# iterate through each day series\n",
    "for daySeries in np.arange(1, 1462, 1).tolist():\n",
    "    \n",
    "    print('getting data for day series ' + str(daySeries))\n",
    "    \n",
    "    # get district counts for selected daySeries\n",
    "    counts_for_selected_day = user_daily_modal_districts.groupBy(str(daySeries)).count().toPandas()\n",
    "    counts_for_selected_day.columns = ['district_id', str(daySeries)]\n",
    "    counts_for_selected_day.dropna(inplace = True)\n",
    "    counts_for_selected_day.district_id = counts_for_selected_day.district_id.astype(int)\n",
    "\n",
    "    # merge counts with results dataset\n",
    "    per_district_per_date_counts = per_district_per_date_counts.merge(counts_for_selected_day, on = 'district_id', how = 'left')\n",
    "\n",
    "per_district_per_date_counts.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>district_id</th>\n",
       "      <th>day_series</th>\n",
       "      <th>user_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>1</td>\n",
       "      <td>331060.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101</td>\n",
       "      <td>2</td>\n",
       "      <td>332718.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101</td>\n",
       "      <td>3</td>\n",
       "      <td>332701.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101</td>\n",
       "      <td>4</td>\n",
       "      <td>332953.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>101</td>\n",
       "      <td>5</td>\n",
       "      <td>306451.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   district_id  day_series  user_count\n",
       "0          101           1    331060.0\n",
       "1          101           2    332718.0\n",
       "2          101           3    332701.0\n",
       "3          101           4    332953.0\n",
       "4          101           5    306451.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_count_per_district_per_day = per_district_per_date_counts.copy()\n",
    "\n",
    "user_count_per_district_per_day.index = per_district_per_date_counts.district_id\n",
    "user_count_per_district_per_day.drop(columns = ['district_id'], inplace = True)\n",
    "\n",
    "user_count_per_district_per_day = user_count_per_district_per_day.stack()\n",
    "user_count_per_district_per_day = user_count_per_district_per_day.reset_index()\n",
    "user_count_per_district_per_day.columns = ['district_id', 'day_series', 'user_count']\n",
    "user_count_per_district_per_day.day_series = user_count_per_district_per_day.day_series.astype(int)\n",
    "\n",
    "user_count_per_district_per_day.sort_values(by=['district_id', 'day_series'], inplace = True)\n",
    "user_count_per_district_per_day.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[401, 402, 614, 708, 816, 1201, 1801, 2902, 3202, 3203]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random_districts = random.sample(user_count_per_district_per_day.district_id.unique().tolist(), 10)\n",
    "random_districts = sorted([402, 1201, 816, 2902, 3203, 1801, 708, 614, 3202, 401])\n",
    "random_districts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "for district in random_districts:\n",
    "    \n",
    "    district_data = user_count_per_district_per_day[(user_count_per_district_per_day.district_id == district)]\n",
    "    ax.plot(district_data.day_series, district_data.user_count)\n",
    "    \n",
    "#ax.set_xticks([1,200,400,600,800,1000,1200])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-smehra_pyspark]",
   "language": "python",
   "name": "conda-env-.conda-smehra_pyspark-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
